# Implementierung & Reflexion (Phase 2)

## Umsetzung des Projekts
Das Projekt wurde als **Microservice-Architektur mit Docker Compose** umgesetzt.  
Die drei zentralen Komponenten sind:
- **PostgreSQL** als persistente Datenbank
- **Redpanda (Kafka)** f√ºr Event Streaming
- **Apache Spark** f√ºr ETL-Verarbeitung und Aggregationen  

Die Container wurden in einem lokalen Docker-Netzwerk orchestriert.  
Ein **Producer-Service** l√§dt Beispieldaten (`bitcoin_sample.csv`) und streamt diese in Kafka.  
Spark konsumiert die Daten, berechnet Tagesdurchschnitte und schreibt die Ergebnisse in die Datenbank.

---

## Orientierung an Zielen / Ressourcen
- Ziel: Aufbau einer skalierbaren, wartbaren Datenpipeline im lokalen Umfeld.  
- Umsetzung mit minimalen Ressourcen (lokaler Laptop, Docker-Container).  
- Gro√üe CSV-Dateien (>100 MB) wurden **nicht ins Git √ºbernommen**. Stattdessen verweist das Repo auf die Rohdatenquelle (Kaggle).  

---

## Probleme & Risiken
- **Gro√üe Datenmengen**: GitHub erlaubt keine Dateien >100 MB ‚Üí L√∂sung: `.gitignore` + Verweis auf externe Datenquelle.  
- **Abh√§ngigkeiten in Spark**: F√ºr Kafka-Integration mussten zus√§tzliche Pakete geladen werden.  
- **Datenkonsistenz**: Duplicate-Key-Fehler in PostgreSQL traten auf, wenn Spark-Jobs mehrfach gestartet wurden.  
  - L√∂sung: Anpassung der Inserts (Upsert/Truncate vor Neuimport w√§re denkbar).  

---

## Status & Fortschritt
- Alle Services laufen lokal zuverl√§ssig in Docker.  
- Datenfluss funktioniert: CSV ‚Üí Producer ‚Üí Kafka ‚Üí Spark ‚Üí PostgreSQL.  
- Ergebnisse sind in der Tabelle `daily_avg` einsehbar.  
- Projektziele (Reliability, Scalability, Maintainability) wurden eingehalten.  
- [ETL Pipeline Screenshot](docs/images/ETL-Pipeline_Screenshot.png)


---

**Hinweis:** Zus√§tzlich zum urspr√ºnglichen Konzept wurde in der Umsetzung **Redpanda** (Kafka-kompatibel) sowie ein kleiner **Python Producer** verwendet.  
Dies war notwendig, um die ETL-Pipeline mit Streaming-Daten zu versorgen und die Kommunikation mit Spark lokal zu validieren, auch wenn dieser Aspekt im urspr√ºnglichen Konzept nicht explizit enthalten war.

---

## Screenshot (Beispiel Docker Setup / Spark Job)
üëâ Hier Screenshot einf√ºgen (`/docs/images/etl_pipeline.png` oder direkt aus VS Code/PebblePad):

---

## Fazit
Die Microservice-Architektur erf√ºllt die Anforderungen der Aufgabenstellung.  
Sie ist modular, erweiterbar und kann bei Bedarf in eine Cloud-Umgebung √ºbertragen werden.  
Besonders herausfordernd war das Handling von externen Datenquellen und die Integration von Spark mit Kafka.  
Durch die iterative Umsetzung konnte ein lauff√§higes und dokumentiertes System erfolgreich aufgebaut werden.

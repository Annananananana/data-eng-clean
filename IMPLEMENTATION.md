# Implementierung & Reflexion (Phase 2)

## Umsetzung des Projekts
Das Projekt wurde als **Microservice-Architektur mit Docker Compose** umgesetzt.  
Die drei zentralen Komponenten sind:
- **PostgreSQL** als persistente Datenbank
- **Redpanda (Kafka)** fÃ¼r Event Streaming
- **Apache Spark** fÃ¼r ETL-Verarbeitung und Aggregationen  

Die Container wurden in einem lokalen Docker-Netzwerk orchestriert.  
Ein **Producer-Service** lÃ¤dt Beispieldaten (`bitcoin_sample.csv`) und streamt diese in Kafka.  
Spark konsumiert die Daten, berechnet Tagesdurchschnitte und schreibt die Ergebnisse in die Datenbank.

---

## Orientierung an Zielen / Ressourcen
- Ziel: Aufbau einer skalierbaren, wartbaren Datenpipeline im lokalen Umfeld.  
- Umsetzung mit minimalen Ressourcen (lokaler Laptop, Docker-Container).  
- GroÃŸe CSV-Dateien (>100 MB) wurden **nicht ins Git Ã¼bernommen**. Stattdessen verweist das Repo auf die Rohdatenquelle (Kaggle).  

---

## Probleme & Risiken
- **GroÃŸe Datenmengen**: GitHub erlaubt keine Dateien >100 MB â†’ LÃ¶sung: `.gitignore` + Verweis auf externe Datenquelle.  
- **AbhÃ¤ngigkeiten in Spark**: FÃ¼r Kafka-Integration mussten zusÃ¤tzliche Pakete geladen werden.  
- **Datenkonsistenz**: Duplicate-Key-Fehler in PostgreSQL traten auf, wenn Spark-Jobs mehrfach gestartet wurden.  
  - LÃ¶sung: Anpassung der Inserts (Upsert/Truncate vor Neuimport wÃ¤re denkbar).  

---

## Status & Fortschritt
- Alle Services laufen lokal zuverlÃ¤ssig in Docker.  
- Datenfluss funktioniert: CSV â†’ Producer â†’ Kafka â†’ Spark â†’ PostgreSQL.  
- Ergebnisse sind in der Tabelle `daily_avg` einsehbar.  
- Projektziele (Reliability, Scalability, Maintainability) wurden eingehalten.  
- [ETL Pipeline Screenshot](docs/images/ETL-Pipeline_Screenshot.png)


---

## Screenshot (Beispiel Docker Setup / Spark Job)
ğŸ‘‰ Hier Screenshot einfÃ¼gen (`/docs/images/etl_pipeline.png` oder direkt aus VS Code/PebblePad):

---

## Fazit
Die Microservice-Architektur erfÃ¼llt die Anforderungen der Aufgabenstellung.  
Sie ist modular, erweiterbar und kann bei Bedarf in eine Cloud-Umgebung Ã¼bertragen werden.  
Besonders herausfordernd war das Handling von externen Datenquellen und die Integration von Spark mit Kafka.  
Durch die iterative Umsetzung konnte ein lauffÃ¤higes und dokumentiertes System erfolgreich aufgebaut werden.

# Data Engineering â€“ Batch Pipeline (Kafka â†’ Spark â†’ Postgres)

## Architektur
- **Ingestion:** Apache Kafka (Redpanda)
- **Processing:** Apache Spark (Batch)
- **Storage:** PostgreSQL
- **Orchestrierung/IaC:** Docker Compose
- **Repro:** Git + GitHub

## Schnellstart
```bash
# 1) Services & einmalige Verarbeitung starten
docker compose up -d postgres redpanda
docker compose build producer
docker compose run --rm producer
docker compose run --rm spark

# 2) Ergebnis prÃ¼fen (Zeilen + min/max Datum)
docker exec -it postgres psql -U user -d bitcoin \
  -c "SELECT COUNT(*) AS rows, MIN(date), MAX(date) FROM daily_avg;"

  ## ðŸ“‚ Daten

Dieses Repository enthÃ¤lt aus PlatzgrÃ¼nden **nicht den vollstÃ¤ndigen historischen Datensatz**, sondern nur eine kleine Beispieldatei:


- Die Datei `bitcoin_sample.csv` ist ausreichend, um den ETL-Workflow und die Architektur zu demonstrieren.  
- Die vollstÃ¤ndige CSV `bitcoin_2017_to_2023.csv` wurde bewusst nicht ins Repository aufgenommen, da sie die GitHub-Grenze von 100 MB Ã¼berschreitet.  
- Falls Sie den gesamten Datensatz nutzen mÃ¶chten, laden Sie ihn bitte manuell herunter und legen Sie ihn im Ordner `data/` ab.

ðŸ‘‰ Quelle der Rohdaten: [Kaggle â€“ Bitcoin Historical Data (2017â€“2023)](https://www.kaggle.com/datasets/jkraak/bitcoin-price-dataset?resource=download)  
*(Bitte beachten: exakter Link je nach Kaggle-Dataset, hier ggf. anpassen!)*

Die `.gitignore`-Konfiguration stellt sicher, dass groÃŸe Rohdaten nicht ins Repo gelangen.

